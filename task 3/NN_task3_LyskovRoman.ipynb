{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLY NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "#         layers\n",
    "        self.input_layer = 2\n",
    "        self.hidden_layer = 3\n",
    "        self.output_layer = 1\n",
    "#         weights\n",
    "        self.W1 = np.random.randn(self.input_layer, self.hidden_layer)\n",
    "        self.W2 = np.random.randn(self.hidden_layer, self.output_layer)\n",
    "#         activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "#         forward propagtion\n",
    "    def forward(self, X):\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "    \n",
    "        return yHat\n",
    "    \n",
    "    def cost_function(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "    \n",
    "        return J\n",
    "    \n",
    "#     backward propagation\n",
    "    def backward(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoid(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T,delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoid(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)\n",
    "#   derivatives of weights\n",
    "        return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()\n",
    "# initialize data\n",
    "X = np.random.rand(1000, 2)\n",
    "y = np.apply_along_axis(lambda element: element[0]+element[1],axis=1,arr=X)\n",
    "y.shape = (1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75.74467324]\n",
      "[78.05995665]\n",
      "[78.81223924]\n",
      "[79.05825912]\n",
      "[79.11711183]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(5):\n",
    "    dJdW1, dJdW2 = NN.backward(X, y)\n",
    "#     update weights\n",
    "    NN.W1 = NN.W1 - learning_rate * dJdW1\n",
    "    NN.W2 = NN.W2 - learning_rate * dJdW2\n",
    "    \n",
    "    print (NN.cost_function(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15823422]\n"
     ]
    }
   ],
   "source": [
    "# print error\n",
    "def mse(y,yHat):\n",
    "    return sum(map(lambda a: pow(a[0]-a[1], 2), zip(y, yHat)))\n",
    "print (mse(y, NN.forward(X))/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
