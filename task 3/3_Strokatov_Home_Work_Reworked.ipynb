{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 Strokatov Home Work",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMaVOKOe/n40JE5nDtZil5m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidDaimond/ds_school_2020/blob/master/task%203/3_Strokatov_Home_Work_Reworked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K46YsT5TF23",
        "colab_type": "code",
        "outputId": "b7c45c92-91c3-48df-f827-6d22b54a6b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from scipy.misc import derivative\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFqFCuK4_qTo",
        "colab_type": "text"
      },
      "source": [
        "# **Data preparations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BDCC83f-y73",
        "colab_type": "code",
        "outputId": "1adfe4cf-90e5-4520-d41e-b580a50daf84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "dfm = pd.read_csv('./tmdb_5000_movies.csv')\n",
        "\n",
        "\n",
        "dfm['release_date'] = pd.to_datetime(dfm['release_date'])\n",
        "dfm['year'] = pd.DatetimeIndex(dfm['release_date']).year\n",
        "dfm['month'] = pd.DatetimeIndex(dfm['release_date']).month\n",
        "dfm['weekday'] = dfm['release_date'].dt.dayofweek\n",
        "\n",
        "\n",
        "dfm.original_language[dfm.original_language != 'en'] = 0\n",
        "dfm.original_language[dfm.original_language != 0] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F9nrkX_CXu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "#Translate data to acceptable format by converting a JSON columns\n",
        "prime_genre = pd.json_normalize(dfm.genres.apply(json.loads).apply(lambda x: x[0] if (x != []) else {'id': 0, 'name': 'None'}))\n",
        "secondary_genre = pd.json_normalize(dfm.genres.apply(json.loads).apply(lambda x: x[1] if (len(x) > 1) else {'id': 0, 'name': 'None'}))\n",
        "\n",
        "#genres\n",
        "dfm['prime_genre_name'] = prime_genre['name']\n",
        "dfm['secondary_genre_name'] = secondary_genre['name']\n",
        "\n",
        "#production country\n",
        "dfm['production_country_id'] = pd.json_normalize(dfm.production_countries.apply(json.loads).apply(lambda x: x[0] if (x != []) else {'iso_3166_1': 'None', 'name': 'Unknown'}))['iso_3166_1']\n",
        "\n",
        "#production companies\n",
        "dfm['production_company_name'] = pd.json_normalize(dfm.production_companies.apply(json.loads).apply(lambda x: x[0] if (x != []) else {'id': 'None', 'name': 'NaN'}))['name']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHwvhLygC0ek",
        "colab_type": "code",
        "outputId": "0724f909-7054-4f55-a937-57bfb6be9b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#New company and genre columns\n",
        "dfm['company_portfolio'] = dfm.groupby('production_company_name')['production_company_name'].transform('count')\n",
        "dfm.production_company_name[dfm.company_portfolio < 2] = 'Noname company'\n",
        "dfm['genre_popularity'] = dfm.groupby('prime_genre_name')['prime_genre_name'].transform('count') + dfm.groupby('secondary_genre_name')['secondary_genre_name'].transform('count')\n",
        "\n",
        "\n",
        "dfm['profitable'] = (dfm.revenue / dfm.budget) > 2.5\n",
        "dfm['foreign'] = (dfm.production_country_id) != 'US'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBG21IBS5F5v",
        "colab_type": "code",
        "outputId": "e7ae3ab9-c13b-4fad-d790-9b3644ca166f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "dfm.genre_popularity.hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3a7583fe80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQmNvmIrFUBD",
        "colab_type": "code",
        "outputId": "b162f408-ecff-4c9f-e772-663d713b1f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dfm = dfm.drop(dfm[(dfm.runtime == 0) | (dfm.revenue == 0) | (dfm.budget == 0) | (dfm.production_company_name == 'NaN')].index, axis=0)\n",
        "dfm.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3188, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPK61NfWzNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfm = dfm.drop(['genres', 'homepage', 'id', 'keywords', 'original_title', 'overview',\n",
        "                'production_companies', 'production_countries', 'spoken_languages', 'tagline',\n",
        "                'title', 'status', 'vote_count', 'popularity'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAHLBy_KX3_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = pd.DataFrame.from_dict({'genres': dfm['prime_genre_name']})\n",
        "b = pd.DataFrame.from_dict({'genres': dfm['secondary_genre_name']})\n",
        "genres = pd.get_dummies(a) + pd.get_dummies(b)\n",
        "\"\"\"pd.get_dummies(dfm[['prime_genre_name', 'secondary_genre_name', \n",
        "                    'production_company_name', \n",
        "                    'production_country_id']]).sample(3)\"\"\"\n",
        "genres = genres.drop(['genres_Foreign', 'genres_None'], axis=1)\n",
        "cat_genres = genres.columns\n",
        "dfm = pd.concat([dfm, genres], axis=1).drop(['prime_genre_name', 'secondary_genre_name'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBEyedQFcX6-",
        "colab_type": "code",
        "outputId": "0ef9df64-818c-4df9-9daa-6c789d50de99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dfm = dfm[dfm.year > 1986]\n",
        "dfm = dfm[np.log(dfm.revenue) > 7.5]\n",
        "dfm.revenue = np.log(dfm.revenue)\n",
        "dfm.budget = np.log(dfm.budget)\n",
        "print(dfm.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2839, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ9CojY6yYdz",
        "colab_type": "text"
      },
      "source": [
        "## Targets and categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5306xBCSleeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_date = ['year', 'month', 'weekday']\n",
        "cat_lang = ['original_language', 'foreign']\n",
        "nums = ['budget', 'runtime', 'vote_average', 'company_portfolio', 'genre_popularity']\n",
        "targets = ['revenue', 'profitable']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu68FW1V_E35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = dfm[cat_date + cat_lang + nums]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5TNtFK0Iuqs",
        "colab_type": "code",
        "outputId": "6df2bb71-c61a-4591-a877-0baed74df1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "X_data.sample(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>weekday</th>\n",
              "      <th>original_language</th>\n",
              "      <th>foreign</th>\n",
              "      <th>budget</th>\n",
              "      <th>runtime</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>company_portfolio</th>\n",
              "      <th>genre_popularity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>2016.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>18.132999</td>\n",
              "      <td>87.0</td>\n",
              "      <td>5.9</td>\n",
              "      <td>260</td>\n",
              "      <td>303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1940</th>\n",
              "      <td>2011.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>17.034386</td>\n",
              "      <td>80.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7</td>\n",
              "      <td>1830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2695</th>\n",
              "      <td>2007.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>16.213406</td>\n",
              "      <td>90.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>63</td>\n",
              "      <td>1419</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        year  month  weekday  ... vote_average  company_portfolio  genre_popularity\n",
              "504   2016.0    6.0      5.0  ...          5.9                260               303\n",
              "1940  2011.0    9.0      4.0  ...          7.0                  7              1830\n",
              "2695  2007.0    3.0      4.0  ...          5.0                 63              1419\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqIV_-2iyH6p",
        "colab_type": "text"
      },
      "source": [
        "## Train-Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbSyN_sWy3hP",
        "colab_type": "code",
        "outputId": "4dc97bfd-1c66-44ad-fe10-938be18239b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "source": [
        "X_norm = X_data[nums].values\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "X_data[nums] = mms.fit_transform(X_norm)\n",
        "\n",
        "y_data = dfm['profitable']\n",
        "X_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[item] = s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>weekday</th>\n",
              "      <th>original_language</th>\n",
              "      <th>foreign</th>\n",
              "      <th>budget</th>\n",
              "      <th>runtime</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>company_portfolio</th>\n",
              "      <th>genre_popularity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0.972950</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>0.847059</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.494797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2007.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0.986456</td>\n",
              "      <td>0.430976</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.403571</td>\n",
              "      <td>0.216178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>0.974852</td>\n",
              "      <td>0.360269</td>\n",
              "      <td>0.741176</td>\n",
              "      <td>0.710714</td>\n",
              "      <td>0.494797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0.976009</td>\n",
              "      <td>0.417508</td>\n",
              "      <td>0.894118</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>0.464049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0.978257</td>\n",
              "      <td>0.306397</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.403571</td>\n",
              "      <td>0.494797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4758</th>\n",
              "      <td>2014.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0.739079</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.003571</td>\n",
              "      <td>0.141438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4773</th>\n",
              "      <td>1994.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0.452699</td>\n",
              "      <td>0.171717</td>\n",
              "      <td>0.870588</td>\n",
              "      <td>0.307143</td>\n",
              "      <td>0.921949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4792</th>\n",
              "      <td>1997.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.435505</td>\n",
              "      <td>0.235690</td>\n",
              "      <td>0.870588</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.148534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>2004.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>0.375353</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.811765</td>\n",
              "      <td>0.010714</td>\n",
              "      <td>0.408231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>1992.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.572895</td>\n",
              "      <td>0.134680</td>\n",
              "      <td>0.776471</td>\n",
              "      <td>0.710714</td>\n",
              "      <td>0.464049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2839 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        year  month  weekday  ... vote_average  company_portfolio  genre_popularity\n",
              "0     2009.0   12.0      3.0  ...     0.847059           0.075000          0.494797\n",
              "1     2007.0    5.0      5.0  ...     0.811765           0.403571          0.216178\n",
              "2     2015.0   10.0      0.0  ...     0.741176           0.710714          0.494797\n",
              "3     2012.0    7.0      0.0  ...     0.894118           0.053571          0.464049\n",
              "4     2012.0    3.0      2.0  ...     0.717647           0.403571          0.494797\n",
              "...      ...    ...      ...  ...          ...                ...               ...\n",
              "4758  2014.0    3.0      5.0  ...     0.682353           0.003571          0.141438\n",
              "4773  1994.0    9.0      1.0  ...     0.870588           0.307143          0.921949\n",
              "4792  1997.0   11.0      3.0  ...     0.870588           0.000000          0.148534\n",
              "4796  2004.0   10.0      4.0  ...     0.811765           0.010714          0.408231\n",
              "4798  1992.0    9.0      4.0  ...     0.776471           0.710714          0.464049\n",
              "\n",
              "[2839 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnxIQoV1AAtN",
        "colab_type": "code",
        "outputId": "6c8e3df0-203b-4fd4-a956-b6baf2ea9309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r_state = 42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data.to_numpy(), \n",
        "                                                    y_data.astype(float).to_numpy(), \n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=r_state)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1987, 10) (852, 10) (1987,) (852,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDj5ulbBSwrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
        "tanh = lambda x: (np.exp(2*x)-1)/(np.exp(2*x)+1)\n",
        "relu = lambda x: x if x > 0 else 0\n",
        "leakyrelu = lambda x: x if x > 0 else 0.1 * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-5jTv6CcCiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    def __init__(self, neurons, inputshape, funcact):\n",
        "        self.wmatrix = np.random.rand(neurons, inputshape)\n",
        "        self.f = funcact\n",
        "        self.inp = None\n",
        "    def forward(self, inp):\n",
        "        self.inp = inp\n",
        "        #print(inp, self.wmatrix)\n",
        "        return [self.f(self.wmatrix[i] @ inp) for i in range(len(self.wmatrix))] + [1] #Forward every neuron + bias, written as [1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNUMkjvl-dB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "    \n",
        "    def add(self, neurons, funcact, inputshape=None):\n",
        "        if inputshape == None:\n",
        "            self.layers += [Layer(neurons, len(self.layers[-1].wmatrix)+1, funcact)]\n",
        "        else:\n",
        "            self.layers += [Layer(neurons, inputshape, funcact)]\n",
        "    \n",
        "    def predict_one(self, X):\n",
        "        for laynum in range(len(self.layers)):\n",
        "            if laynum == 0:\n",
        "                nextdoor = self.layers[laynum].forward(X)\n",
        "            else:\n",
        "                nextdoor = self.layers[laynum].forward(nextdoor)\n",
        "        return nextdoor[:1]\n",
        "    \n",
        "    def predict(self, X):\n",
        "        ans = np.array([])\n",
        "        for x in X:\n",
        "            ans = np.concatenate((ans, self.predict_one(x)), axis=0)\n",
        "        return ans.reshape(len(ans), 1)\n",
        "\n",
        "    def fit(self, X, y, num_epochs=10, lr=0.01):\n",
        "        ans = self.predict(X)\n",
        "        for epoch in range(num_epochs):\n",
        "            for answer in range(len(y)):\n",
        "                #First time we count deltas\n",
        "                for layer in range(len(self.layers)):\n",
        "                    if layer == 0:\n",
        "                        self.layers[-(layer + 1)].delta = y[answer] - ans[answer]\n",
        "                    #for pre-last layer count of delta have a little difference, because last layer doesn't have bias\n",
        "                    elif layer == 1:\n",
        "                        #delta of dense can be interpreted as weights matrix mult with vector of delta of next layer\n",
        "                        self.layers[-(layer + 1)].delta = self.layers[-layer].wmatrix.T @ self.layers[-layer].delta\n",
        "                        \n",
        "                        #reshape vector\n",
        "                        self.layers[-(layer + 1)].delta = self.layers[-(layer + 1)].delta.reshape(len(self.layers[-(layer + 1)].delta), 1)\n",
        "\n",
        "                    else:\n",
        "                        self.layers[-(layer + 1)].delta = self.layers[-layer].wmatrix.T @ self.layers[-layer].delta[:-1]\n",
        "                        #reshape vector\n",
        "                        self.layers[-(layer + 1)].delta = self.layers[-(layer + 1)].delta.reshape(len(self.layers[-(layer + 1)].delta), 1)\n",
        "                #Then we change a weights\n",
        "                for layer in range(len(self.layers)):\n",
        "                    #A little desciption: \n",
        "                    #deriv - Derivative matrix of layer. It's just a little list comprehension, it's not scary\n",
        "                    #out - Matrix of inputs of weights\n",
        "                    #grad - is the matrix of gradients, WgradAB = deltaB * f'()\n",
        "                    \n",
        "                    #for the first layer we have a little bit different weight changing\n",
        "                    if layer == 0:\n",
        "\n",
        "                        deriv = np.array([derivative(self.layers[layer].f, self.layers[layer].forward(self.layers[layer].inp)[i]) for i in range(len(self.layers[layer].forward(self.layers[layer].inp)))][:-1])\n",
        "                        #In the first time we use input as OUT\n",
        "                        out = X[answer]\n",
        "                        grad = np.array([lr * self.layers[layer].delta.ravel()[:-1] * deriv * out[i] for i in range(len(out))])\n",
        "                        self.layers[layer].wmatrix = self.layers[layer].wmatrix - grad.T\n",
        "                    \n",
        "                    #also for pre-last, because last layer doesn't have bias neuron\n",
        "                    elif layer == len(self.layers) - 1:\n",
        "                        deriv = np.array([derivative(self.layers[layer].f, self.layers[layer].forward(self.layers[layer].inp)[i]) for i in range(len(self.layers[layer].forward(self.layers[layer].inp)))][:-1])\n",
        "                        out = self.layers[layer-1].forward(self.layers[layer-1].inp)\n",
        "                        grad = np.array([lr * self.layers[layer].delta.ravel() * deriv * out[i] for i in range(len(out))])\n",
        "                        self.layers[layer].wmatrix = self.layers[layer].wmatrix - grad.T     \n",
        "                    \n",
        "                    #for others just use this\n",
        "                    else:\n",
        "                        deriv = np.array([derivative(self.layers[layer].f, self.layers[layer].forward(self.layers[layer].inp)[i]) for i in range(len(self.layers[layer].forward(self.layers[layer].inp)))][:-1])\n",
        "                        out = self.layers[layer-1].forward(self.layers[layer-1].inp)\n",
        "                        grad = np.array([lr * self.layers[layer].delta.ravel()[:-1] * deriv * out[i] for i in range(len(out))])\n",
        "                        self.layers[layer].wmatrix = self.layers[layer].wmatrix - grad.T\n",
        "            print(\"Epoch was completed\")                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm_FcEyBEzWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perc = Perceptron()\n",
        "perc.add(64, sigmoid, 10)\n",
        "perc.add(128, sigmoid)\n",
        "perc.add(1, sigmoid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66FjN6KiU1hQ",
        "colab_type": "code",
        "outputId": "6c7f47e1-e7be-44a0-e5d0-3286cb97d2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "perc.predict(X_test).ravel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-GjW-2aVK5c",
        "colab_type": "code",
        "outputId": "f601ef3a-0762-4901-a182-39fa13e4be0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "%%time\n",
        "perc.fit(X_train, y_train, num_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "Epoch was completed\n",
            "CPU times: user 1h 7min 53s, sys: 2.01 s, total: 1h 7min 55s\n",
            "Wall time: 1h 7min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GZcnMtDldrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As I counted, fit process made 180 598 430 iterations!\n",
        "#((10*64)+(65*128)+129*1)weights * 10 epochs * 1987 objects from data_train\n",
        "#Now predict!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9VAyYk8mkEo",
        "colab_type": "code",
        "outputId": "990cffcd-8aae-421d-e0df-64d986eeae21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "perc.predict(X_test).ravel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8DkH2WjmsQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#...\n",
        "#Directed by David Strokatov..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njud3B-1ynX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    return (y_true==y_pred).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1YrGKOv5hsg",
        "colab_type": "code",
        "outputId": "9938b386-f087-4a77-e0f8-69e1eb5e6dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(perc.predict(X_test), y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4413145539906103"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    }
  ]
}